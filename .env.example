# QueryMate (.env.example)
# Copy this file to ".env" and adjust as needed.
# This project uses Ollama locally with an OpenAI-compatible endpoint.

APP_ENV=local

# --- Ollama settings ---
# If you changed the Ollama port/host, update LLM_BASE accordingly (e.g., 11435).
LLM_PROVIDER=ollama
LLM_BASE=http://localhost:11434/v1/chat/completions

# Pick an installed model (check with: ollama list)
LLM_MODEL=llama3.1:latest
# Examples:
# LLM_MODEL=llama3.1:8b-instruct
# LLM_MODEL=llama3.1:70b-instruct
